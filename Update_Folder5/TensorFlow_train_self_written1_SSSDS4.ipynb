{"cells":[{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":186,"status":"ok","timestamp":1674035026845,"user":{"displayName":"Zihao LIU","userId":"01384449044500819690"},"user_tz":0},"id":"P8-ZYy5QY9yH"},"outputs":[],"source":["# 我们要定位文件位置，使用这里的代码，具体要将此notebook文件和function文件（或文件夹）放在同一个文件夹路径下，具体路径是什么可以左侧找到目标文件夹然后右击复制文件夹路径来获得\n","import sys\n","sys.path.insert(0,'/content/drive/MyDrive/JPMorgan/自写代码/TensorFlow_codes')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OwvisHXMxdXn"},"outputs":[],"source":["pip install einops"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OAVTt6V-9tow"},"outputs":[],"source":["pip install wandb"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GH48gfRY9vVW"},"outputs":[],"source":["pip install pytorch_lightning"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PwkQMpvX9xNC"},"outputs":[],"source":["pip install pykeops"]},{"cell_type":"code","execution_count":20,"metadata":{"executionInfo":{"elapsed":7672,"status":"ok","timestamp":1674035071717,"user":{"displayName":"Zihao LIU","userId":"01384449044500819690"},"user_tz":0},"id":"KSoo7pZPi1qW"},"outputs":[],"source":["pip install -q -U tensorflow-addons"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9772,"status":"ok","timestamp":1672849451456,"user":{"displayName":"Zihao LIU","userId":"01384449044500819690"},"user_tz":-480},"id":"gYr-Y-SV56wl","outputId":"22a60f0b-be32-43b3-f6fc-282f15d0ff36"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":21,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1674035071717,"user":{"displayName":"Zihao LIU","userId":"01384449044500819690"},"user_tz":0},"id":"E3vheVLoZmn8"},"outputs":[],"source":["import os\n","import argparse\n","import json\n","import numpy as np\n","#------------------------------------------------------------------\n","import tensorflow as tf\n","tf.config.run_functions_eagerly(True)\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","from tensorflow_probability.python.internal import tensor_util\n","from tensorflow.keras.models import Sequential\n","#from keras.models import load_model\n","import torch\n","#import torch.nn as nn\n","#------------------------------------------------------------------\n","from utils.util import find_max_epoch, print_size, training_loss, calc_diffusion_hyperparams, std_normal\n","from utils.util import get_mask_mnr, get_mask_bm, get_mask_rm\n","\n","from imputers.DiffWaveImputer import DiffWaveImputer\n","from imputers.SSSDSAImputer import SSSDSAImputer\n","from imputers.SSSDS4Imputer import SSSDS4Imputer\n","\n","import easydict"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":561,"status":"ok","timestamp":1673705093627,"user":{"displayName":"Zihao LIU","userId":"01384449044500819690"},"user_tz":-480},"id":"s7iOhY8Dc1NY","outputId":"4c42d6f5-3019-4856-d3e6-b0ea7b11ea14"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'2.9.2'"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["#Check that you have the last version of tensorflow\n","tf.__version__"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2096,"status":"ok","timestamp":1673539098580,"user":{"displayName":"Zihao LIU","userId":"01384449044500819690"},"user_tz":-480},"id":"Idk68Knp64Th","outputId":"3d390ad1-6787-494f-ad2c-bbcb950f7f49"},"outputs":[{"data":{"text/plain":["(8000, 100, 14)"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["# We can check the train data's dimensions. It is a 3-D dataset\n","# Mujoco 数据中，第一维度(0): 8000代表8000个时间序列，或者8000次实验轨道。因为此数据集一共10,000个序列，根据80%作为train数据，20%作为test数据而分配到了8000次实验的train数据\n","# 第二维度(1): 100表示每次实验中的100个时间点(time points)\n","# 第三维度(2): 14表示14个features\n","# 上述分析均来自target paper原文中对Mujoco数据的解释\n","data = np.load(\"/content/drive/MyDrive/JPMorgan/自写代码/Data/Mujoco/train_mujoco.npy\")\n","data.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1673539098580,"user":{"displayName":"Zihao LIU","userId":"01384449044500819690"},"user_tz":-480},"id":"Gqch6sRA4iTD","outputId":"920ecfc0-906a-4ac5-8782-d9534a5dbbd6"},"outputs":[{"data":{"text/plain":["(100, 14)"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["data.shape[1:]"]},{"cell_type":"code","execution_count":22,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1674035071718,"user":{"displayName":"Zihao LIU","userId":"01384449044500819690"},"user_tz":0},"id":"rZjck2XzTCNn"},"outputs":[],"source":["class CustomFit(keras.Model):\n","  def __init__(self,model,only_generate_missing,loss_mask):\n","    super(CustomFit,self).__init__()\n","    self.model = model\n","    self.only_generate_missing = only_generate_missing\n","    self.loss_mask = loss_mask\n","\n","  def compile(self, optimizer): #loss\n","    super(CustomFit, self).compile()\n","    self.optimizer = optimizer\n","    #self.loss = loss\n","\n","\n","  def train_step(self, data):\n","    x, y = data\n","\n","    with tf.GradientTape() as tape:\n","      y_pred = self.model(x , training = True)\n","      if self.only_generate_missing == 1:\n","        loss_mask = torch.tensor(self.loss_mask.numpy(),dtype=torch.bool)\n","        y_pred = torch.tensor(y_pred.numpy())\n","        y_pred = tf.convert_to_tensor(y_pred[loss_mask].numpy().reshape((50,1260)),dtype=tf.float32)\n","        loss = keras.losses.mean_squared_error(y, y_pred)\n","      elif self.only_generate_missing == 0:\n","        #y_pred = tf.reshape(y_pred,[50,1260])\n","        loss = keras.losses.mean_squared_error(y, y_pred)\n","\n","    #loss = loss\n","    training_vars = self.trainable_variables\n","    gradients = tape.gradient(loss, training_vars)\n","    #print(\"loss_function:++++++++++:\",loss)\n","    #print(\"training_vars:+++++++++:\", training_vars) # Add\n","    #print(\"gradients:+++++++++:\", gradients) # Add\n","\n","    self.optimizer.apply_gradients(zip(gradients, training_vars))\n","    acc_metric = tf.keras.metrics.MeanSquaredError(name=\"mse\")\n","    acc_metric.update_state(y, y_pred)\n","    #self.compiled_metrics.update_state(y, y_pred)\n","\n","    return {\"loss\":loss, \"mse\":acc_metric.result()}\n","\n","\n"]},{"cell_type":"code","execution_count":23,"metadata":{"executionInfo":{"elapsed":298,"status":"ok","timestamp":1674035072007,"user":{"displayName":"Zihao LIU","userId":"01384449044500819690"},"user_tz":0},"id":"-t0qKUmfaGu6"},"outputs":[],"source":["def train(output_directory,\n","          ckpt_iter,\n","          n_iters,\n","          iters_per_ckpt,\n","          iters_per_logging,\n","          learning_rate,\n","          use_model,\n","          only_generate_missing,\n","          masking,\n","          missing_k):\n","    \n","    \"\"\"\n","    Train Diffusion Models\n","\n","    Parameters:\n","    output_directory (str):         save model checkpoints to this path\n","    ckpt_iter (int or 'max'):       the pretrained checkpoint to be loaded; \n","                                    automatically selects the maximum iteration if 'max' is selected\n","    data_path (str):                path to dataset, numpy array.\n","    n_iters (int):                  number of iterations to train\n","    iters_per_ckpt (int):           number of iterations to save checkpoint, \n","                                    default is 10k, for models with residual_channel=64 this number can be larger\n","    iters_per_logging (int):        number of iterations to save training log and compute validation loss, default is 100\n","    learning_rate (float):          learning rate\n","\n","    use_model (int):                0:DiffWave. 1:SSSDSA. 2:SSSDS4.\n","    only_generate_missing (int):    0:all sample diffusion.  1:only apply diffusion to missing portions of the signal\n","    masking(str):                   'mnr': missing not at random, 'bm': blackout missing, 'rm': random missing\n","    missing_k (int):                k missing time steps for each feature across the sample length.\n","    \"\"\"\n","\n","    # generate experiment (local) path\n","    local_path = \"T{}_beta0{}_betaT{}\".format(diffusion_config[\"T\"],\n","                                              diffusion_config[\"beta_0\"],\n","                                              diffusion_config[\"beta_T\"]) \n","\n","    # Get shared output_directory ready\n","    output_directory = os.path.join(output_directory, local_path)\n","    if not os.path.isdir(output_directory):\n","        os.makedirs(output_directory)\n","        os.chmod(output_directory, 0o775)\n","    print(\"output directory\", output_directory, flush=True)\n","\n","    # map diffusion hyperparameters to gpu\n","    global diffusion_hyperparams  # Add\n","    for key in diffusion_hyperparams:\n","        if key != \"T\":\n","            diffusion_hyperparams[key] = diffusion_hyperparams[key] #[Zihao]: 这里从util.py获得超参数后，对于所有的不是T的超参数(e.g., \"Beta\",\"Alpha\"),都分配到gpu上，即记为cuda\n","\n","    # predefine model\n","    if use_model == 0:\n","        net = DiffWaveImputer(**model_config)  # [Zihao]: 这里引入了imputers文件夹内的提前预定好的三个模型：[0] CSDI, [1] SSSDSA [2] sssds4， net:torch network\n","    elif use_model == 1:                  # 这里的**model_config表示在不同imputers下的提前预定好的模型参数，不同imputer的参数略有不同，可以看\n","        net = SSSDSAImputer(**model_config)   # 下面的一个代码块内定义了：global model_config，获得的参数来自各自imputer模型的json结构文件中\n","    elif use_model == 2:                  # 的 'wavenet_config' (对SSSDSA模型是\"sashimi_config\")，在下面的代码块内会指定imputer json结构文件的路径来获得参数\n","        net = SSSDS4Imputer(**model_config)   # 这里的net是nn.Model,是一种torch模型\n","    else:\n","        print('Model chosen not available.')\n","    print_size(net)\n","\n","    # <***>define optimizer\n","    #optimizer = tf.keras.optimizers.Adam(net.parameters(), lr=learning_rate)                       #[Zihao]: torch used\n","                                                            # 这是 Adam算法，一种随机最优化算法，给出可迭代的参数与学习率，进行converge，这里给出的\n","                                                            # 参数就是net.parameters(), 这是特定imputer模型的初始化参数，net在前面被定义为了一个torch model(已转为cuda),这是pytorch框架下的model,model.parameters()就会返回此model所有参数list\n","                                                            # lr是Adam算法的学习率参数，我们规定为learning_rate,来自于\n","                                                            # 对应imputer的json文件中的\"train_config\"\n","    # load checkpoint\n","    if ckpt_iter == 'max':          # 我们在imputer的json文件中设置的就是max，所以checkpoint会首先从我们储存的output_directory中寻找之前存储的检查点，避免花费大量时间继续构建结构\n","        ckpt_iter = find_max_epoch(output_directory)    # find_max_epoch来自util.py文件里面的class\n","    if ckpt_iter >= 0:            # 如果找到了存储在output_directory中的检查点：\n","        try:\n","            #net2 = Sequential()  # 我们在这里设定模型类型是Squential()\n","            # load file\n","            model_path = os.path.join(output_directory, '{}.weight'.format(ckpt_iter))  # 这里需要包括.h5 file名字，比如'models/model.h5', models是一个文件夹，path结尾是h5文件\n","            # feed the model with all config and weights\n","            model = net\n","            model.load_weights(model_path, by_name=False)                                                                             \n","            # net2.compile(loss=loss, optimizer=tf.keras.optimizers.Adam(lr=learning_rate), metrics=[\"mse\"])  # 这里的loss=loss第二个loss是我们下面设定好的loss function给出的loss值\n","\n","            print('Successfully loaded model at iteration {}'.format(ckpt_iter))           # 到这里才算是将checkpoint里面所有参数重新读取到新的模型中了\n","        except:\n","            ckpt_iter = -1\n","            print('No valid checkpoint model found, start training from initialization try.')   # 如果没有找到checkpoint，意味着要从头训练模型结构，这时可以看到手动将ckpt_iter设定为-1，为的是下面的train过程，不同的ckpt_iter会有不同的结果\n","    else:\n","        ckpt_iter = -1\n","        print('No valid checkpoint model found, start training from initialization.')\n","\n","        \n","        \n","    \n","    ### Custom data loading and reshaping ###\n","        \n","        \n","    training_data = np.load(trainset_config['train_data_path'])  # 'train_data_path'        # trainset_config 在下面的初始化里面有，表示从imputer的json结构中获得train data的路径\n","    training_data = np.split(training_data, 160, 0)     # 这个将会把 training_data 分为160个相等分段(batch)，0表现axis=0，沿着横轴(时间)分割，training_data是3-D矩阵数据，第一维度是实验轨道次数：8000(0)，第二维度是每个轨道的100个样本时间点(1), 第三维度是14个features(2),所以这里表示分割为160份数据，features数目保持不变，时间保持不变，轨道分为160份，每份8000/160个轨道实验\n","    training_data = np.array(training_data)          # 转为array\n","    training_data = tensor_util.convert_nonref_to_tensor(training_data)          # tf.convert_to_tensor             # 转为TF tensor, TF默认使用GPU环境进行运算所以不需要额外声明\n","    print('Data loaded')                                             \n","\n","    # training\n","    n_iter = ckpt_iter + 1                 # 这里的train.py的训练过程，如果在上面已经找到了之前训练过的checkpoint,则在上面已经赋给model了checkpoint内的神经网络层的参数，比如SSSDS4模型我们之前设定的\"n_iters\": 500 (在json文件中)，产生的checkpoint有600.pkl\n","                                  # 那么这里的ckpt_iter = 600, 则n_iter = 601>下面的n_iters+1 = 501 所以直接不进行while循环，直接跳出不再训练。\n","                                  # 如果在上面的checkpoint内没有发现.pkl记忆点文件，则上面会赋值ckpt_iter = -1, 此时n_iter = 0,则会进行下面的while循环进行训练，每次训练结束会给n_iter+1,当循环500次的时候，末尾再加1变成n_iter=501 = n_iters + 1(我们设定的),此时跳出循环完成训练\n","    while n_iter < n_iters + 1:\n","        for batch in training_data:           # 在上面我们将training_data分为了160组数据，那么应该就是160个batch，进行批量训练\n","            print(\"This is loop: \", n_iter,\"...\")\n","            #print('batch[0]: ', batch[0])\n","\n","            if masking == 'rm':           # 这里就是看missing的数据类型是什么，可以是'rm': random missing, 'mnr': missing not at random, 'bm': blackout missing, 在SSSDS4里我们目前设定的是rm\n","                transposed_mask = get_mask_rm(batch[0], missing_k)  # get_mask_rm从util.py中import的,样本为batch[0]，这是随便选的第一个batch为例子，维度只有时间和feature两个, 因为在util.py里面我们只需要用到sample.shape，shape的大小数值而已\n","            elif masking == 'mnr':\n","                transposed_mask = get_mask_mnr(batch[0], missing_k)  # get_mask_mnr从util.py中import的,样本为batch[0]，这是随便选的第一个batch为例子，维度只有时间和feature两个, 因为在util.py里面我们只需要用到sample.shape，shape的大小数值而已\n","            elif masking == 'bm':\n","                transposed_mask = get_mask_bm(batch[0], missing_k)  # get_mask_bm从util.py中import的,样本为batch[0]，这是随便选的第一个batch为例子，维度只有时间和feature两个, 因为在util.py里面我们只需要用到sample.shape，shape的大小数值而已\n","\n","            mask = tf.transpose(transposed_mask, perm=[1, 0])    # transposed_mask是一个tensor,这个tf.transpose()method是将此tensor的维度转为transposed_mask原始维度(0,1)中，1变到现在的第一号位，0变到现在的第二号位置: (1,0),其实就是行变列，列变行，现在行有14个表示features，列有100个表示每个轨道的时间点            \n","            mask = tf.expand_dims(mask, axis=0)    # 增加一个维度，原本mask是二维，现在在位置0增加一个维度，值为1，也就是增加了batch的第一维度\n","            mask = tf.repeat(mask, repeats=batch.shape[0],axis = 0)    # 表示将原始mask重复，构成新的tensor,结构设定为重复batch第一维度：轨道数，以此来补充满每个batch内的所有轨道数: 8000/160,每列每行中一个元素: 0 or 1，batch是一个三维数组，后两维都保持不变，于是这里说明我们直接将mask加了一个维度即batch.size()[0](第一维度)\n","            loss_mask = tensor_util.convert_nonref_to_tensor(~np.array(mask,dtype=bool),dtype=tf.int32)  # tf.convert_to_tensor  #将mask的值都转为bool值，0与1，之后使用~运算符表示：按位取反，本来是0的改为1，本来是1的改为0，之后叫做loss_mask\n","              \n","            batch = tf.transpose(batch, perm=[0, 2, 1])    # 这里表示把batch的维度的后两个维度替换位置，现在第一维度轨道数保持不变，第二维度变成features，第三维度变成每个轨道的时间点数，此时和mask的三个维度重合\n","            \n","            print(\"Check shapes: \",batch.shape, mask.shape, loss_mask.shape)\n","            assert batch.shape == mask.shape == loss_mask.shape  # assert是一个检查维护过程，这里检查batch, mask和loss_mask的size(三个维度大小)是否都相等\n","\n","            # back-propagation\n","            X_train = batch, batch, mask, loss_mask  # 这里在输入要计算loss function的自变量X, X[0] = batch, X[1] = batch, X[2] = mask. X[3] = loss_mask\n","            \n","            #loss, Y_train = training_loss(net, tf.keras.losses.MeanSquaredError(), X_train, diffusion_hyperparams,         # 这里使用的training_loss类在util.py文件中，返回的是loss值，即nn.MSE值，diffusion_hyperparams是计算出来的，来自util.py的calc_diffusion_hyperparams,在nn.MSE(a,b)中，a是预测值, b是真实值\n","            #                     only_generate_missing=only_generate_missing)  # 注意最后的only_generate_missing，看上面的介绍，这个值如果是0则表示使用所有样本进行diffusion后获得MSE，如果是1则表示只使用missing部分进行diffusion获得的MSE\n","            #==============================================================\n","            model = net\n","            loss_fn = tf.keras.losses.MeanSquaredError()\n","            X = X_train\n","            diffusion_hyperparams = diffusion_hyperparams\n","            only_generate_missing=only_generate_missing\n","            #----------------------------------------------------\n","            _dh = diffusion_hyperparams     # 计算出的扩散模型超参数\n","            T, Alpha_bar = _dh[\"T\"], _dh[\"Alpha_bar\"]            \n","            audio = X[0]\n","            cond = X[1]\n","            mask = X[2]\n","            loss_mask = X[3]\n","\n","            B, C, L = audio.shape  # B is batchsize, C=1, L is audio length\n","            diffusion_steps = tf.random.uniform(minval=0, maxval=T, shape=(B, 1, 1), dtype=tf.int32)  # randomly sample diffusion steps from 1~T (random integers)\n","            diffusion_steps = diffusion_steps.numpy()\n","            z = std_normal(audio.shape)\n","            if only_generate_missing == 1:\n","                z = audio * mask + z * (1 - mask)\n","\n","            #print(Alpha_bar)\n","            \n","            transformed_X = tf.math.sqrt(Alpha_bar[diffusion_steps]) * audio + tf.math.sqrt(1 - Alpha_bar[diffusion_steps]) * z  # compute x_t from q(x_t|x_0)\n","            diffusion_steps = tensor_util.convert_nonref_to_tensor(diffusion_steps,dtype=tf.int32) # tf.convert_to_tensor\n","            print(transformed_X.shape)\n","            \n","            epsilon_theta = model((transformed_X, cond, mask, tf.reshape(diffusion_steps,[B, 1])))  # predict \\epsilon according to \\epsilon_\\theta\n","            \n","            if only_generate_missing == 1:\n","              loss_mask = torch.tensor(loss_mask.numpy(),dtype=torch.bool)\n","              epsilon_theta = torch.tensor(epsilon_theta.numpy())\n","              epsilon_theta = tf.convert_to_tensor(epsilon_theta[loss_mask].numpy().reshape((50,1260)),dtype=tf.float32) # tf.convert_to_tensor #tensor_util.convert_nonref_to_tensor\n","              #print('This is epsilon_theta: ++++++++++++++++++',epsilon_theta)\n","              \n","            #elif only_generate_missing == 0:\n","            #  epsilon_theta = tf.convert_to_tensor(epsilon_theta.numpy(),dtype=tf.float32)\n","\n","\n","            \n","            #print('====+++++++====== epsilon_theta: ', epsilon_theta)\n","            #print(z.shape)  # Add\n","            \n","            #print(cond.shape)\n","            #print(mask.shape)\n","            #print(tf.reshape(diffusion_steps,[B, 1]).shape)\n","            #print(loss_mask.shape)\n","            #print(epsilon_theta.shape)\n","            #print(loss_mask)\n","            #print(len(z))\n","            #print([only_generate_missing])\n","\n","            #z = torch.tensor(z.numpy())  # 这里转为torch为了下面的loss_fn(z[loss_mask],epsilon_theta[loss_mask])\n","            loss_mask = torch.tensor(loss_mask.numpy(),dtype=torch.bool)\n","            #epsilon_theta = torch.tensor(epsilon_theta.numpy())\n","            \n","            print(model)  # 这里的net还是SSSD4模型\n","            model.trainable = True\n","            #print(\"trainable_weights:----------------------------------------------------------------------------------------------------\", net.get_weights())\n","\n","            #print(lzh)\n","            #net2 = Sequential([net])\n","            #net2 = keras.Model(inputs=inputs, outputs=outputs)  #这里我们定义net为Sequential()模型，为了下面的compile\n","\n","            #print('This is input epsilon_theta: ',tf.convert_to_tensor(epsilon_theta[loss_mask].numpy()))\n","            #print('This is input z: ',tf.convert_to_tensor(z[loss_mask].numpy().reshape((50,1260))))\n","            #loss_mask2 = X[3]\n","            x_train = (transformed_X, cond, mask, tf.reshape(diffusion_steps,[B, 1])) # loss_mask2, tf.constant([only_generate_missing]*transformed_X.shape[0])\n","            \n","\n","            if only_generate_missing == 1: \n","              #net.compile(loss=loss_fn, optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate), metrics=[\"mse\"], run_eagerly=True)  # 这里的loss=loss第二个loss是我们上面设定好的loss function给出的loss值\n","              #print(tf.convert_to_tensor(epsilon_theta[loss_mask].numpy()).shape)\n","              #print(len(np.stack((transformed_X, cond, mask, tf.reshape(diffusion_steps,[B, 1])),axis=0)))\n","              #print(len(np.stack(z[loss_mask].numpy(),axis=0)))\n","              #input = transformed_X, cond, mask, tf.reshape(diffusion_steps,[B, 1])\n","              #print('TYPE: ',type(input))net\n","              y_train = tf.convert_to_tensor(z[loss_mask].numpy().reshape((50,1260)))\n","\n","              training = CustomFit(model=model, only_generate_missing=only_generate_missing, loss_mask=X[3])\n","              training.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate)) # \n","              training.fit(x_train,y_train,batch_size = 50, epochs = 1)\n","            elif only_generate_missing == 0:\n","              y_train = z\n","              \n","              training = CustomFit(model=model, only_generate_missing=only_generate_missing, loss_mask=X[3])\n","              training.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate)) # \n","              training.fit(x_train,y_train,batch_size = 50, epochs = 1)\n","              #net.compile(loss=tf.keras.losses.MeanSquaredError(), optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate), metrics=[\"mse\"])  # 这里的loss=loss第二个loss是我们上面设定好的loss function给出的loss值\n","              #history = net.fit(tf.convert_to_tensor(epsilon_theta.numpy()),tf.convert_to_tensor(z.numpy()))\n","            #==============================================================\n","\n","            if n_iter % iters_per_logging == 0:   # 默认是100，即当迭代次数是整百次的时候，我们这里print出已经进行了多少次迭代和loss值是多少\n","                print(\"iteration: {} \".format(n_iter))\n","\n","            # save checkpoint\n","            if n_iter > 0 and n_iter % iters_per_ckpt == 0:  # 在进行了整百次训练的时候：\n","                #checkpoint_name1 = '{}.config'.format(n_iter)  #这是在给checkpoint文件(.pkl)进行命名,由于.h5在TF框架下无法存储subclassed的客户定制模型，我们随便起一个名字叫.h5py来存储\n","                checkpoint_name_weight = '{}.weight'.format(n_iter)\n","                \n","                #json_str = model.to_json()\n","                #with open(os.path.join(output_directory, checkpoint_name1),'w') as text_file:\n","                #  text_file.write(json_str)\n","                model.save_weights(os.path.join(output_directory, checkpoint_name_weight))\n","\n","                #model.save(os.path.join(output_directory, checkpoint_name),save_format='tf')  # 存储路径为json文件内定义的output_directory，命名为checkpoint的文件名\n","                print('model weight at iteration %s is saved===============================' % n_iter)  # print出checkpoint以及完成储存\n","\n","            n_iter += 1   # 迭代次数+1，进行下一次的迭代\n","\n","   "]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1674035072846,"user":{"displayName":"Zihao LIU","userId":"01384449044500819690"},"user_tz":0},"id":"FddY4GWp3xt_","outputId":"8042ccbf-ab6b-4ee0-f3c2-39f8f7b2f007"},"outputs":[{"output_type":"stream","name":"stdout","text":["{'diffusion_config': {'T': 200, 'beta_0': 0.0001, 'beta_T': 0.02}, 'wavenet_config': {'in_channels': 14, 'out_channels': 14, 'num_res_layers': 36, 'res_channels': 256, 'skip_channels': 256, 'diffusion_step_embed_dim_in': 128, 'diffusion_step_embed_dim_mid': 512, 'diffusion_step_embed_dim_out': 512, 's4_lmax': 100, 's4_d_state': 64, 's4_dropout': 0.0, 's4_bidirectional': 1, 's4_layernorm': 1}, 'train_config': {'output_directory': '/content/drive/MyDrive/JPMorgan/自写代码/TensorFlow_codes/Results_SSSDS4/Mujoco/train_90/', 'ckpt_iter': 'max', 'iters_per_ckpt': 100, 'iters_per_logging': 100, 'n_iters': 300, 'learning_rate': 0.0002, 'only_generate_missing': 0, 'use_model': 2, 'masking': 'rm', 'missing_k': 90}, 'trainset_config': {'train_data_path': '/content/drive/MyDrive/JPMorgan/自写代码/Data/Mujoco/train_mujoco.npy', 'test_data_path': '/content/drive/MyDrive/JPMorgan/自写代码/Data/Mujoco/test_mujoco.npy', 'segment_length': 100, 'sampling_rate': 100}, 'gen_config': {'output_directory': '/content/drive/MyDrive/JPMorgan/自写代码/TensorFlow_codes/Results_SSSDS4/Mujoco/test_90/', 'ckpt_path': '/content/drive/MyDrive/JPMorgan/自写代码/TensorFlow_codes/Results_SSSDS4/Mujoco/train_90/'}}\n"]}],"source":["if __name__ == \"__main__\":                  # __name__ == \"__main__\" 表示目前run的就是train_self_written1_SSSDS4.ipynb文件本身，使用的模型(即train，上面block里定义的)都来自这个文件,这个代码块代表train()模型初始化时候的给定参数\n","    parser = argparse.ArgumentParser()\n","    parser.add_argument('-c', '--config', type=str, default='/content/drive/MyDrive/JPMorgan/自写代码/TensorFlow_codes/config/config_SSSDS4.json',  \n","                        help='JSON file for configuration')  # 这里只是表示读取文件的位置等，当run这个block的时候显示的比较好看\n","\n","    args = parser.parse_args(\"\")\n","\n","    with open(args.config) as f:   # 读取json参数内容\n","        data = f.read()   # 读取json参数内容\n","\n","    config = json.loads(data)  # 读取json参数内容， 将变量config赋值为json形式下的data\n","    print(config)   \n","\n","    train_config = config[\"train_config\"]  # training parameters  #训练模型的参数，来自对应imputer的json参数\n","\n","    global trainset_config\n","    trainset_config = config[\"trainset_config\"]  # to load trainset  # train和test数据的路径参数，来自对应imputer的json参数\n","\n","    global diffusion_config\n","    diffusion_config = config[\"diffusion_config\"]  # basic hyperparameters  # 这是diffusion模型的初始化超参数，来自对应imputer的json参数，之后会带入util.py文件的calc_diffusion_hyperparams去计算出扩散模型的超参数\n","\n","    global diffusion_hyperparams\n","    diffusion_hyperparams = calc_diffusion_hyperparams(\n","        **diffusion_config)  # dictionary of all diffusion hyperparameters  # 这里就是使用了util.py文件内的calc_diffusion_hyperparams,引入所有diffusion_config内的参数去计算出了扩散模型的超参数\n","\n","    global model_config\n","    if train_config['use_model'] == 0:         # 这些是imputer对应模型的初始化超参数，分别对应0:DiffWave. 1:SSSDSA. 2:SSSDS4.\n","        model_config = config['wavenet_config']   # 给model_config赋值这些参数，之后可以带入模型进行imputer初始化，方便后续使用Adam算法进行迭代，见上面\n","    elif train_config['use_model'] == 1:\n","        model_config = config['sashimi_config']\n","    elif train_config['use_model'] == 2:\n","        model_config = config['wavenet_config']\n"]},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a176XBiv4yqS","outputId":"be01ea17-1c24-4611-8fe3-c3e8212db25a","executionInfo":{"status":"ok","timestamp":1674035076367,"user_tz":0,"elapsed":1186,"user":{"displayName":"Zihao LIU","userId":"01384449044500819690"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["output directory /content/drive/MyDrive/JPMorgan/自写代码/TensorFlow_codes/Results_SSSDS4/Mujoco/train_90/T200_beta00.0001_betaT0.02\n","Successfully loaded model at iteration 300\n","Data loaded\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n","WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).init_conv.conv.kernel\n","WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).init_conv.conv.bias\n","WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).residual_layer.fc_t1.kernel\n","WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).residual_layer.fc_t1.bias\n","WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).residual_layer.fc_t2.kernel\n","WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).residual_layer.fc_t2.bias\n","WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).residual_layer.residual_blocks.res_conv.kernel\n","WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).residual_layer.residual_blocks.res_conv.bias\n","WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).residual_layer.residual_blocks.skip_conv.kernel\n","WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).residual_layer.residual_blocks.skip_conv.bias\n","WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).final_conv.layer_with_weights-0.conv.kernel\n","WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).final_conv.layer_with_weights-0.conv.bias\n","WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).final_conv.layer_with_weights-1.conv.kernel\n","WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).final_conv.layer_with_weights-1.conv.bias\n","WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).residual_layer.residual_blocks.conv_layer.conv.kernel\n","WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).residual_layer.residual_blocks.conv_layer.conv.bias\n","WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).residual_layer.residual_blocks.cond_conv.conv.kernel\n","WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).residual_layer.residual_blocks.cond_conv.conv.bias\n"]}],"source":["# The start of the training.\n","# All the parameters are the already set ones in **train_config, it is from the json file we set, \n","# here is \"/content/drive/MyDrive/JPMorgan/自写代码/TensorFlow_codes/config/config_SSSDS4.json\",\n","# the \"**\" means the \"train\" model will accept all the parameters in the json file above. So by this way,\n","# all the already defined parameters in the json file will by inputted into the \"train\" model \n","\n","# It will take lots of time when you first run it as it will generate the iteration saving checkpoint. So next\n","# time when you run it, it will just start from the last iteration memory checkpoint. The path you can find is\n","# in the json file called: \"output_directory\"\n","\n","train(**train_config)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fXZO93Bk4ytB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1674024915747,"user_tz":0,"elapsed":2903,"user":{"displayName":"Zihao LIU","userId":"01384449044500819690"}},"outputId":"04b74cfd-19d7-4c50-8dac-945af704c8eb"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f655cdb4ac0>"]},"metadata":{},"execution_count":12}],"source":["net = SSSDS4Imputer(**model_config)\n","net.load_weights('/content/drive/MyDrive/JPMorgan/自写代码/TensorFlow_codes/Results_SSSDS4/Mujoco/train_90/T200_beta00.0001_betaT0.02/1.weight',by_name=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d0CFLHGJ4y4_"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E_w_NwWg4y7m"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":472,"status":"ok","timestamp":1668792214586,"user":{"displayName":"Zihao LIU","userId":"01384449044500819690"},"user_tz":0},"id":"HyluzmfJpld7","outputId":"a2b2a816-926e-4d8e-8afb-4bae9c814465"},"outputs":[{"name":"stdout","output_type":"stream","text":["Fri Nov 18 17:23:34 2022       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   55C    P8    10W /  70W |      0MiB / 15109MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}],"source":["# Test whether GPU is using\n","!nvidia-smi"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3867,"status":"ok","timestamp":1671319172096,"user":{"displayName":"Zihao LIU","userId":"01384449044500819690"},"user_tz":0},"id":"DgjFqSXq3Umu","outputId":"04d930d5-5168-47e4-d0fd-a32d2ba21bd2"},"outputs":[{"data":{"text/plain":["True"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["torch.cuda.is_available()\n"]},{"cell_type":"markdown","metadata":{"id":"a50FB58cz1zu"},"source":["————————————————————————————————————————\n","\n","The following is done by myself used for some exercises\n","\n","————————————————————————————————————————"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":590,"status":"ok","timestamp":1672996523647,"user":{"displayName":"Zihao LIU","userId":"01384449044500819690"},"user_tz":-480},"id":"KazGNnNHL5dE","outputId":"f373c1fb-6207-4a01-a6dc-ba09183b240c"},"outputs":[{"data":{"text/plain":["<tf.Tensor: shape=(50, 1260), dtype=int64, numpy=\n","array([[    0,     1,     2, ...,  1257,  1258,  1259],\n","       [ 1260,  1261,  1262, ...,  2517,  2518,  2519],\n","       [ 2520,  2521,  2522, ...,  3777,  3778,  3779],\n","       ...,\n","       [59220, 59221, 59222, ..., 60477, 60478, 60479],\n","       [60480, 60481, 60482, ..., 61737, 61738, 61739],\n","       [61740, 61741, 61742, ..., 62997, 62998, 62999]])>"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["a = np.arange(63000).reshape((50, 1260))\n","b = tf.convert_to_tensor(a)\n","b"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1672996526239,"user":{"displayName":"Zihao LIU","userId":"01384449044500819690"},"user_tz":-480},"id":"KJlvWSbGXBwM","outputId":"128cf031-0dfa-49b6-b33e-4ce1901ceaa3"},"outputs":[{"data":{"text/plain":["<tf.Tensor: shape=(), dtype=int64, numpy=0>"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["c = np.arange(63000).reshape((50, 1260))\n","d = tf.convert_to_tensor(c)\n","mse = tf.keras.losses.MeanSquaredError()\n","mse(b,d)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":505,"status":"ok","timestamp":1672994277231,"user":{"displayName":"Zihao LIU","userId":"01384449044500819690"},"user_tz":-480},"id":"hSHvO6apq273","outputId":"fc7d9dce-f068-4070-9449-aa63bf6ebde9"},"outputs":[{"data":{"text/plain":["<tf.Tensor: shape=(), dtype=bool, numpy=True>"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["import tensorflow as tf\n","tf.constant([1]*10)[0]==1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oRNfaPyTQC0V"},"outputs":[],"source":["import numpy as np\n","import tensorflow as tf\n","import tensorflow_probability as tfp\n","aaa = tf.ones([100,14])\n","bbb = tf.expand_dims(aaa, axis=0)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1672621903169,"user":{"displayName":"Zihao LIU","userId":"01384449044500819690"},"user_tz":-480},"id":"l1XqfskdCAIg","outputId":"7911302e-2727-4b39-8a8f-69d959caf42d"},"outputs":[{"data":{"text/plain":["100"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["aaa.shape[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1672622129793,"user":{"displayName":"Zihao LIU","userId":"01384449044500819690"},"user_tz":-480},"id":"Dowx6pFoDBg5","outputId":"695b696c-6125-493a-b8d4-1bd2012fa6bd"},"outputs":[{"data":{"text/plain":["array([[1., 1., 1., ..., 1., 1., 1.],\n","       [1., 1., 1., ..., 1., 1., 1.],\n","       [1., 1., 1., ..., 1., 1., 1.],\n","       ...,\n","       [1., 1., 1., ..., 1., 1., 1.],\n","       [1., 1., 1., ..., 1., 1., 1.],\n","       [1., 1., 1., ..., 1., 1., 1.]], dtype=float32)"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["aaa.numpy()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1671412303670,"user":{"displayName":"Zihao LIU","userId":"01384449044500819690"},"user_tz":0},"id":"ImiL4u1m7t4s","outputId":"9f660f1f-b2c4-409a-f5f3-1baecb3deec4"},"outputs":[{"data":{"text/plain":["<tf.Tensor: shape=(), dtype=int32, numpy=9>"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["tf.keras.losses.mean_squared_error([5],[8])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":218,"status":"ok","timestamp":1671403259402,"user":{"displayName":"Zihao LIU","userId":"01384449044500819690"},"user_tz":0},"id":"WcXPXrIeYtO4","outputId":"cf1b1b29-1d37-49ae-f0a6-28ea29c67a6e"},"outputs":[{"data":{"text/plain":["array([ True, False, False,  True, False,  True])"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["mask2=~np.array(mask,dtype=bool)\n","mask2"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1671403275673,"user":{"displayName":"Zihao LIU","userId":"01384449044500819690"},"user_tz":0},"id":"8-rL9n6OZdbk","outputId":"6bcc9474-cbd0-4b42-e0ed-57f7e303074b"},"outputs":[{"data":{"text/plain":["<tf.Tensor: shape=(6,), dtype=bool, numpy=array([ True, False, False,  True, False,  True])>"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["tf.convert_to_tensor(mask2)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1113,"status":"ok","timestamp":1672621914787,"user":{"displayName":"Zihao LIU","userId":"01384449044500819690"},"user_tz":-480},"id":"58_Uem6GOZTf","outputId":"79783ea4-3ccd-4730-884c-cdd16db5af37"},"outputs":[{"data":{"text/plain":["100"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["import torch\n","mask2 = torch.ones([100,14])\n","mask2.size(0)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1672368542163,"user":{"displayName":"Zihao LIU","userId":"01384449044500819690"},"user_tz":-480},"id":"hKe4E3dxj-xX","outputId":"83ff412c-14c2-4784-e35b-4c807201bbbe"},"outputs":[{"data":{"text/plain":["tensor([[ 0.7820,  0.1527],\n","        [-0.4065,  0.0887],\n","        [-0.6603, -1.4359]], device='cuda:0')"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["def std_normal(size):\n","    \"\"\"\n","    Generate the standard Gaussian variable of a certain size\n","    \"\"\"\n","\n","    return torch.normal(0, 1, size=size).cuda()\n","z = std_normal((3,2))\n","z"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1672368935192,"user":{"displayName":"Zihao LIU","userId":"01384449044500819690"},"user_tz":-480},"id":"gP3MUME07as7","outputId":"d64a19fe-8936-49c1-c8ee-85f3b80d22c6"},"outputs":[{"data":{"text/plain":["tensor([[[ 0.7820,  0.1527],\n","         [-0.6603, -1.4359]],\n","\n","        [[ 0.7820,  0.1527],\n","         [ 0.7820,  0.1527]],\n","\n","        [[-0.4065,  0.0887],\n","         [-0.4065,  0.0887]]], device='cuda:0')"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["index = [[0,2],[0,0],[1,1]]\n","aaa = torch.tensor(index)\n","z[aaa]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1672368804097,"user":{"displayName":"Zihao LIU","userId":"01384449044500819690"},"user_tz":-480},"id":"9eWS_jOu8nYd","outputId":"ff5958d6-8582-41ed-ce6d-60e2c8d45e56"},"outputs":[{"data":{"text/plain":["torch.Size([3, 2])"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["aaa.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4481,"status":"ok","timestamp":1672369035155,"user":{"displayName":"Zihao LIU","userId":"01384449044500819690"},"user_tz":-480},"id":"KPXM0Lww9aCL","outputId":"715998c3-6d88-42d3-99d3-1fe4954de7f3"},"outputs":[{"data":{"text/plain":["<tf.Tensor: shape=(3, 2), dtype=float32, numpy=\n","array([[ 0.519981  , -0.28209618],\n","       [ 0.01872117, -1.0729865 ],\n","       [ 0.32988086,  0.2622309 ]], dtype=float32)>"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["import tensorflow as tf\n","def std_normal2(size):\n","    \"\"\"\n","    Generate the standard Gaussian variable of a certain size\n","    \"\"\"\n","\n","    return tf.random.normal(mean=0, stddev=1, shape=size)\n","z2 = std_normal2((3,2))\n","z2"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":366},"executionInfo":{"elapsed":5,"status":"error","timestamp":1672369208714,"user":{"displayName":"Zihao LIU","userId":"01384449044500819690"},"user_tz":-480},"id":"hniZLYOU9aGg","outputId":"1ed07788-15a7-42d0-dfe1-017693250e63"},"outputs":[{"ename":"TypeError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-25-ae5edc6d07a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mbbb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbbb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36m_check_index\u001b[0;34m(idx)\u001b[0m\n\u001b[1;32m    905\u001b[0m     \u001b[0;31m# TODO(slebedev): IndexError seems more appropriate here, but it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    906\u001b[0m     \u001b[0;31m# will break `_slice_helper` contract.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 907\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_SLICE_TYPE_ERROR\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\", got {!r}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    908\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    909\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: Only integers, slices (`:`), ellipsis (`...`), tf.newaxis (`None`) and scalar tf.int32/tf.int64 tensors are valid indices, got <tf.Tensor: shape=(2,), dtype=int32, numpy=array([0, 2], dtype=int32)>"]}],"source":["bbb = tf.convert_to_tensor(index)\n","for i in range(len(z2)):\n","  print(z2[i][bbb[i]])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":467,"status":"ok","timestamp":1672369431576,"user":{"displayName":"Zihao LIU","userId":"01384449044500819690"},"user_tz":-480},"id":"ZOWx4ajQ9aKt","outputId":"a3f00429-2b92-4840-ba4e-0b8baa9c76d0"},"outputs":[{"data":{"text/plain":["<tf.Tensor: shape=(2,), dtype=float32, numpy=array([0.32988086, 0.2622309 ], dtype=float32)>"]},"execution_count":32,"metadata":{},"output_type":"execute_result"}],"source":["z2[2]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1672369351363,"user":{"displayName":"Zihao LIU","userId":"01384449044500819690"},"user_tz":-480},"id":"w8OKD4l-iXJz","outputId":"8889cf1c-8bd6-40a6-d5fc-f5d5e3c6198c"},"outputs":[{"data":{"text/plain":["<tf.Tensor: shape=(), dtype=int32, numpy=0>"]},"execution_count":29,"metadata":{},"output_type":"execute_result"}],"source":["bbb[0][0]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":853,"status":"ok","timestamp":1672143856872,"user":{"displayName":"Zihao LIU","userId":"01384449044500819690"},"user_tz":-480},"id":"sgN1BjkwNu0O","outputId":"efa90146-f688-4c2f-b4db-11981d5bdee3"},"outputs":[{"data":{"text/plain":["torch.Size([14, 100])"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["mask3.size()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1671401542007,"user":{"displayName":"Zihao LIU","userId":"01384449044500819690"},"user_tz":0},"id":"GUWVve_8KgFc","outputId":"57140367-907a-4fb5-bb5c-18fb98cc299f"},"outputs":[{"data":{"text/plain":["array([4, 2, 0], dtype=int32)"]},"execution_count":92,"metadata":{},"output_type":"execute_result"}],"source":["aaa=tf.random.shuffle(range(5))[0:3].numpy()\n","aaa"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1671397707958,"user":{"displayName":"Zihao LIU","userId":"01384449044500819690"},"user_tz":0},"id":"8B1dSRtmELJ0","outputId":"5eea6462-625a-4037-c4c1-c3960fba4245"},"outputs":[{"data":{"text/plain":["tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["torch.tensor(range(10))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":399,"status":"ok","timestamp":1671397625392,"user":{"displayName":"Zihao LIU","userId":"01384449044500819690"},"user_tz":0},"id":"5aTBTQYXY54d","outputId":"c08fd046-0147-4904-b40a-771319c23dbc"},"outputs":[{"data":{"text/plain":["tensor([4, 1, 2, 5, 6, 9, 7, 8, 3, 0])"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["import torch\n","torch.randperm(len(torch.tensor(range(10))))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1671218571726,"user":{"displayName":"Zihao LIU","userId":"01384449044500819690"},"user_tz":0},"id":"rcIThDExQTf2","outputId":"e6969056-2010-4623-bf50-73c63319f54c"},"outputs":[{"name":"stdout","output_type":"stream","text":["aaa (2, 3)\n","bbb (1, 2, 3)\n","ccc (4, 2, 3)\n"]}],"source":["print('aaa',aaa.shape)\n","print('bbb',bbb.shape)\n","print('ccc',ccc.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2768,"status":"ok","timestamp":1671230903283,"user":{"displayName":"Zihao LIU","userId":"01384449044500819690"},"user_tz":0},"id":"RtAWGpDHy9vx","outputId":"ea77355a-fa7b-45c2-eac2-b4ae8f6a96f8"},"outputs":[{"data":{"text/plain":["tensor([[[1, 2, 3],\n","         [4, 5, 6]],\n","\n","        [[1, 2, 3],\n","         [4, 5, 6]],\n","\n","        [[1, 2, 3],\n","         [4, 5, 6]],\n","\n","        [[1, 2, 3],\n","         [4, 5, 6]]])"]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":[" import torch\n"," x = torch.tensor([[1, 2, 3],[4,5,6]])\n"," y = x.repeat(4, 1,1)\n"," y"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YzFkJ_QzH-EU"},"outputs":[],"source":["import tensorflow as tf\n","aaa=tf.convert_to_tensor([[1,2,3],[2,3,5]])\n","#aaa.numpy()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1672144291489,"user":{"displayName":"Zihao LIU","userId":"01384449044500819690"},"user_tz":-480},"id":"TAtCHZ7pX5B1","outputId":"0a1e2ccf-5ede-462d-fc60-15be92520d4d"},"outputs":[{"data":{"text/plain":["3"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["aaa.shape[-1]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":402,"status":"ok","timestamp":1671217991506,"user":{"displayName":"Zihao LIU","userId":"01384449044500819690"},"user_tz":0},"id":"DAxT0sabV72N","outputId":"678ed54c-1425-459b-da0f-ba19c699d5ce"},"outputs":[{"name":"stdout","output_type":"stream","text":["x torch.Size([2, 3])\n","y torch.Size([4, 2, 3])\n"]}],"source":["print('x',x.shape)\n","print('y',y.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":219,"status":"ok","timestamp":1670101431399,"user":{"displayName":"Zihao LIU","userId":"01384449044500819690"},"user_tz":0},"id":"3F9AIS5JzGhl","outputId":"06605224-f5a2-4405-c66c-9c686e4e4807"},"outputs":[{"data":{"text/plain":["tensor([[2, 3, 5, 2, 3, 5, 2, 3, 5, 2, 3, 5, 2, 3, 5],\n","        [2, 3, 5, 2, 3, 5, 2, 3, 5, 2, 3, 5, 2, 3, 5],\n","        [2, 3, 5, 2, 3, 5, 2, 3, 5, 2, 3, 5, 2, 3, 5]])"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["aaa.repeat(3,5)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1670101438253,"user":{"displayName":"Zihao LIU","userId":"01384449044500819690"},"user_tz":0},"id":"jel_uPfVzIlz","outputId":"8d082063-fc7d-4517-fd12-6c3441dbd42b"},"outputs":[{"data":{"text/plain":["tensor([[[2, 3, 5, 2, 3, 5]],\n","\n","        [[2, 3, 5, 2, 3, 5]],\n","\n","        [[2, 3, 5, 2, 3, 5]]])"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["aaa.repeat(3, 1, 2)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1670101490210,"user":{"displayName":"Zihao LIU","userId":"01384449044500819690"},"user_tz":0},"id":"oEM0RAQ8zYw_","outputId":"01504969-7734-4621-f28a-82e09e4f92b1"},"outputs":[{"data":{"text/plain":["torch.Size([3, 1, 6])"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["aaa.repeat(3, 1, 2).size()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1670101982052,"user":{"displayName":"Zihao LIU","userId":"01384449044500819690"},"user_tz":0},"id":"goBpTvobzlh_","outputId":"c525bcc0-0d19-44bc-9013-f633ce8b9c77"},"outputs":[{"data":{"text/plain":["3"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["aaa.repeat(3,5).shape[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RYwMqGKk1mKA"},"outputs":[],"source":["data = np.load(\"/content/drive/MyDrive/JPMorgan/自写代码/Data/Mujoco/train_mujoco.npy\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u-tnB17F6p7M"},"outputs":[],"source":["data"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":454,"status":"ok","timestamp":1672141784129,"user":{"displayName":"Zihao LIU","userId":"01384449044500819690"},"user_tz":-480},"id":"GOPxW0ta6rgU","outputId":"60a91725-138a-4dfa-96f8-5cc2b539c9e5"},"outputs":[{"data":{"text/plain":["tensor([[-0.9415,  0.7483,  0.6444],\n","        [-0.1961,  0.2103, -1.0184]])"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["\n","import torch\n","x = torch.randn(2, 3)\n","x"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1672141885368,"user":{"displayName":"Zihao LIU","userId":"01384449044500819690"},"user_tz":-480},"id":"Oodx7i5Nw0DF","outputId":"52fb0989-356a-4607-8cf7-c80a257a8f01"},"outputs":[{"data":{"text/plain":["tensor([[[-0.9415],\n","         [ 0.7483]],\n","\n","        [[ 0.6444],\n","         [-0.1961]],\n","\n","        [[ 0.2103],\n","         [-1.0184]]])"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["x.view([3,2,1])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1672141939399,"user":{"displayName":"Zihao LIU","userId":"01384449044500819690"},"user_tz":-480},"id":"DF4fAaHQlZMo","outputId":"25302080-a409-47b8-c785-dc625903c214"},"outputs":[{"data":{"text/plain":["tensor([[[-0.9415],\n","         [ 0.7483]],\n","\n","        [[ 0.6444],\n","         [-0.1961]],\n","\n","        [[ 0.2103],\n","         [-1.0184]]])"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["x.reshape([3,2,1])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1672030269675,"user":{"displayName":"Zihao LIU","userId":"01384449044500819690"},"user_tz":-480},"id":"NwN7NfW0xOY3","outputId":"ffa19126-4237-4dee-bbb9-a50422c6a77c"},"outputs":[{"data":{"text/plain":["<tf.Tensor: shape=(2, 9), dtype=float32, numpy=\n","array([[-0.5305082 , -1.3207436 , -0.753209  , -0.5305082 , -1.3207436 ,\n","        -0.753209  , -0.5305082 , -1.3207436 , -0.753209  ],\n","       [ 0.47132334, -0.8071756 ,  0.13752413,  0.47132334, -0.8071756 ,\n","         0.13752413,  0.47132334, -0.8071756 ,  0.13752413]],\n","      dtype=float32)>"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["tf.concat((y,y,y),1)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1672142183087,"user":{"displayName":"Zihao LIU","userId":"01384449044500819690"},"user_tz":-480},"id":"691YClM1l0Kp","outputId":"cc7459cb-7443-44de-d159-7fb8393ef81f"},"outputs":[{"data":{"text/plain":["<tf.Tensor: shape=(3, 2, 1), dtype=int32, numpy=\n","array([[[1],\n","        [2]],\n","\n","       [[3],\n","        [2]],\n","\n","       [[3],\n","        [5]]], dtype=int32)>"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["import tensorflow as tf\n","aaa=tf.convert_to_tensor([[1,2,3],[2,3,5]])\n","bbb=tf.reshape(aaa,[3,2,1])\n","bbb"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LDCLJsZPtu-R"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"mount_file_id":"1uh4w_dGBvuN3ube8x3qZnwUhkVwN1OtP","authorship_tag":"ABX9TyOLvnSf/jWnZbmfihNrBXaN"},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}