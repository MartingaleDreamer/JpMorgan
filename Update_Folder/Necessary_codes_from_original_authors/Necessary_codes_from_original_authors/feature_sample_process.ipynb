{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"imWFp5M0hCww"},"outputs":[],"source":["import numpy as np\n","from sklearn.preprocessing import StandardScaler"],"id":"imWFp5M0hCww"},{"cell_type":"code","execution_count":null,"metadata":{"id":"awFltUsrhCwx"},"outputs":[],"source":["# pems bay example  # 这是一个样本取样过程，进行批量学习的时候需要按照份数进行取样，每一份数据包含一定量的时间点L和一定量的features(K)\n","\n","dataset = np.load('path to raw dataset after downloaded') \n","dataset = dataset[50000,325] # 50000 L, 325 K 注: 一共50000行(L：time points)，325列 (K：features)\n","dataset = np.array(np.split(dataset, 250, 0)) # 250 N, 200 L, 325 K. 注:这里的250 N代表有多少份数据，这里是共250份数据， 250N*200L = 50000L\n","train = dataset[0:240] # 240 份数据用于train\n","test = dataset[240:250]  # 10份数据用于test\n","scaler = StandardScaler().fit(train.reshape(-1, train.shape[-1]))  # 标准化train, reshape(-1)表示将function内未知维度数据转化为1D数据, train.shape[-1]表示train的列有多少列，所以scaler转为了一行，train那么多列\n","train_scaled = scaler.transform(train.reshape(-1, train.shape[-1])).reshape(train.shape)  # transform返回一个自己对自己使用的function所以这边只是将train set标准化后\n","test_scaled = scaler.transform(test.reshape(-1, test.shape[-1])).reshape(test.shape)  # 这边同理是将test set标准化后\n","train_scaled_fs = np.array(np.split(train_scaled, 5, 2)).transpose(0,1,3,2) # 5 B, 240 N, 65 K, 200 L. #在train数据集325个K中中再分出5份，每份有65个K，这里的5B就是均分为5份, split是沿着axis=2（K）进行分割分成6份，transpose(0,1,3,2)是将之前标准化的train阵保持之前的在(0,1,3,2)(共4维)维度上的维度重新生成transpose后的维持了之前(0,1,3,2)各自维度的新阵，L还是200表示200个时间点\n","\n","\n","fs_batch1 = train_scaled_fs[:,0:40,:,:]  # 由于按照上面说的transpose(0,1,3,2)保持了原始各位置上的维度，这里0:40划分的位置是原始的1位置的维度，应该是train的份数，，每个batch都有40份的意思，每一份是200时间点，65个features（K）\n","fs_batch2 = train_scaled_fs[:,40:80,:,:]  # 如上所说将240份均匀划分为6个batch进行批量训练\n","fs_batch3 = train_scaled_fs[:,80:120,:,:]\n","fs_batch4 = train_scaled_fs[:,120:160,:,:]\n","fs_batch5 = train_scaled_fs[:,160:200,:,:]\n","fs_batch6 = train_scaled_fs[:,200:240,:,:]\n","\n","\n","# training train.py\n","n_iter = ckpt_iter + 1  # 这个ckpt_iter前面没有定义，这个是用户自己设置的迭代次数，在/content/drive/MyDrive/JPMorgan/SSSD-main/src/config/config_DiffWave.json里面有设置，这里的while循环说的就是当小于迭代次数时候则进行批量学习，每一个batch都要循环一遍\n","while n_iter < n_iters + 1:\n","    for batch_fs in zip(fs_batch1,fs_batch2,fs_batch3,fs_batch4,fs_batch5,fs_batch6): # 5 B, 40 N, 65 K, 200 L #一共6个batch，每个batch 40份数据，65个features(K)，200个时间点(L)\n","        for batch in batch_d: # 40 N, 65 K, 200 L  (batch into the model)\n","            \n","            \n","# same principle for testing/inference.\n","\n","# to compute metrics: \n","# 0) concatenate all batches if want to compute all together, oterwise compute metric per batch\n","# 1) transpose back the data to N, L, K: if numpy use transpose and concatenate\n","# 2) inverse transform the scaled data\n","# 3) compute metrics... "],"id":"awFltUsrhCwx"},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NmcHt_WqhV_q","executionInfo":{"status":"ok","timestamp":1668452566875,"user_tz":0,"elapsed":17896,"user":{"displayName":"Zihao LIU","userId":"01384449044500819690"}},"outputId":"f0a5b39d-efe3-41bb-a8a8-8f94afc31ceb"},"id":"NmcHt_WqhV_q","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WGdDkWfnhCwy"},"outputs":[],"source":[],"id":"WGdDkWfnhCwy"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}