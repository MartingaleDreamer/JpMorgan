{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1uh4w_dGBvuN3ube8x3qZnwUhkVwN1OtP","authorship_tag":"ABX9TyOu8USGAMiQw9++thh6FhSh"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard","accelerator":"GPU"},"cells":[{"cell_type":"code","source":["# 我们要定位文件位置，使用这里的代码，具体要将此notebook文件和function文件（或文件夹）放在同一个文件夹路径下，具体路径是什么可以左侧找到目标文件夹然后右击复制文件夹路径来获得\n","import sys\n","sys.path.insert(0,'/content/drive/MyDrive/JPMorgan/自写代码/TensorFlow_codes')"],"metadata":{"id":"P8-ZYy5QY9yH","executionInfo":{"status":"ok","timestamp":1670538321613,"user_tz":0,"elapsed":213,"user":{"displayName":"Zihao LIU","userId":"01384449044500819690"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["pip install einops"],"metadata":{"id":"OwvisHXMxdXn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pip install wandb"],"metadata":{"id":"cYSzyUAFxj31"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pip install pytorch_lightning"],"metadata":{"id":"tTBC0A48xuEG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pip install pykeops"],"metadata":{"id":"ow6HdCihL2Tu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gYr-Y-SV56wl","executionInfo":{"status":"ok","timestamp":1670103311508,"user_tz":0,"elapsed":12333,"user":{"displayName":"Zihao LIU","userId":"01384449044500819690"}},"outputId":"d9d0563a-052f-46d6-c6e2-1e235c153de8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["import os\n","import argparse\n","import json\n","import numpy as np\n","#------------------------------------------------------------------\n","import tensorflow as tf\n","#import torch\n","#import torch.nn as nn\n","#------------------------------------------------------------------\n","from utils.util import find_max_epoch, print_size, training_loss, calc_diffusion_hyperparams\n","from utils.util import get_mask_mnr, get_mask_bm, get_mask_rm\n","\n","from imputers.DiffWaveImputer import DiffWaveImputer\n","from imputers.SSSDSAImputer import SSSDSAImputer\n","from imputers.SSSDS4Imputer import SSSDS4Imputer\n","\n","import easydict"],"metadata":{"id":"E3vheVLoZmn8","executionInfo":{"status":"ok","timestamp":1670431063102,"user_tz":0,"elapsed":3861,"user":{"displayName":"Zihao LIU","userId":"01384449044500819690"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["#Check that you have the last version of tensorflow\n","tf.__version__"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"s7iOhY8Dc1NY","executionInfo":{"status":"ok","timestamp":1670431080923,"user_tz":0,"elapsed":5,"user":{"displayName":"Zihao LIU","userId":"01384449044500819690"}},"outputId":"d286f475-7255-4459-8c25-6a0231d9838b"},"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'2.9.2'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":15}]},{"cell_type":"code","source":["# We can check the train data's dimensions. It is a 3-D dataset\n","# Mujoco 数据中，第一维度(0): 8000代表8000个时间序列，或者8000次实验轨道。因为此数据集一共10,000个序列，根据80%作为train数据，20%作为test数据而分配到了8000次实验的train数据\n","# 第二维度(1): 100表示每次实验中的100个时间点(time points)\n","# 第三维度(2): 14表示14个features\n","# 上述分析均来自target paper原文中对Mujoco数据的解释\n","data = np.load(\"/content/drive/MyDrive/JPMorgan/自写代码/Data/Mujoco/train_mujoco.npy\")\n","data.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Idk68Knp64Th","executionInfo":{"status":"ok","timestamp":1670430674723,"user_tz":0,"elapsed":1820,"user":{"displayName":"Zihao LIU","userId":"01384449044500819690"}},"outputId":"8480c141-e539-4403-d9b9-5b0f8148b42c"},"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(8000, 100, 14)"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["def train(output_directory,\n","          ckpt_iter,\n","          n_iters,\n","          iters_per_ckpt,\n","          iters_per_logging,\n","          learning_rate,\n","          use_model,\n","          only_generate_missing,\n","          masking,\n","          missing_k):\n","    \n","    \"\"\"\n","    Train Diffusion Models\n","\n","    Parameters:\n","    output_directory (str):         save model checkpoints to this path\n","    ckpt_iter (int or 'max'):       the pretrained checkpoint to be loaded; \n","                                    automatically selects the maximum iteration if 'max' is selected\n","    data_path (str):                path to dataset, numpy array.\n","    n_iters (int):                  number of iterations to train\n","    iters_per_ckpt (int):           number of iterations to save checkpoint, \n","                                    default is 10k, for models with residual_channel=64 this number can be larger\n","    iters_per_logging (int):        number of iterations to save training log and compute validation loss, default is 100\n","    learning_rate (float):          learning rate\n","\n","    use_model (int):                0:DiffWave. 1:SSSDSA. 2:SSSDS4.\n","    only_generate_missing (int):    0:all sample diffusion.  1:only apply diffusion to missing portions of the signal\n","    masking(str):                   'mnr': missing not at random, 'bm': blackout missing, 'rm': random missing\n","    missing_k (int):                k missing time steps for each feature across the sample length.\n","    \"\"\"\n","\n","    # generate experiment (local) path\n","    local_path = \"T{}_beta0{}_betaT{}\".format(diffusion_config[\"T\"],\n","                                              diffusion_config[\"beta_0\"],\n","                                              diffusion_config[\"beta_T\"]) \n","\n","    # Get shared output_directory ready\n","    output_directory = os.path.join(output_directory, local_path)\n","    if not os.path.isdir(output_directory):\n","        os.makedirs(output_directory)\n","        os.chmod(output_directory, 0o775)\n","    print(\"output directory\", output_directory, flush=True)\n","\n","    # map diffusion hyperparameters to gpu\n","    for key in diffusion_hyperparams:\n","        if key != \"T\":\n","            diffusion_hyperparams[key] = diffusion_hyperparams[key].cuda() #[Zihao]: 这里从util.py获得超参数后，对于所有的不是T的超参数(e.g., \"Beta\",\"Alpha\"),都分配到gpu上，即记为cuda\n","\n","    # predefine model\n","    if use_model == 0:\n","        net = DiffWaveImputer(**model_config).cuda()  # [Zihao]: 这里引入了imputers文件夹内的提前预定好的三个模型：[0] CSDI, [1] SSSDSA [2] sssds4， net:torch network\n","    elif use_model == 1:                  # 这里的**model_config表示在不同imputers下的提前预定好的模型参数，不同imputer的参数略有不同，可以看\n","        net = SSSDSAImputer(**model_config).cuda()   # 下面的一个代码块内定义了：global model_config，获得的参数来自各自imputer模型的json结构文件中\n","    elif use_model == 2:                  # 的 'wavenet_config' (对SSSDSA模型是\"sashimi_config\")，在下面的代码块内会指定imputer json结构文件的路径来获得参数\n","        net = SSSDS4Imputer(**model_config).cuda()   # 这里的net是nn.Model,是一种torch模型\n","    else:\n","        print('Model chosen not available.')\n","    print_size(net)\n","\n","    # <***>define optimizer\n","    optimizer = tf.keras.optimizers.Adam(net.parameters(), lr=learning_rate)                       #[Zihao]: torch used\n","                                                            # 这是 Adam算法，一种随机最优化算法，给出可迭代的参数与学习率，进行converge，这里给出的\n","                                                            # 参数就是net.parameters(), 这是特定imputer模型的初始化参数，net在前面被定义为了一个torch model(已转为cuda),这是pytorch框架下的model,model.parameters()就会返回此model所有参数list\n","                                                            # lr是Adam算法的学习率参数，我们规定为learning_rate,来自于\n","                                                            # 对应imputer的json文件中的\"train_config\"\n","    # load checkpoint\n","    if ckpt_iter == 'max':          # 我们在imputer的json文件中设置的就是max，所以checkpoint会首先从我们储存的output_directory中寻找之前存储的检查点，避免花费大量时间继续构建结构\n","        ckpt_iter = find_max_epoch(output_directory)    # find_max_epoch来自util.py文件里面的class\n","    if ckpt_iter >= 0:            # 如果找到了存储在output_directory中的检查点：\n","        try:\n","            # load checkpoint file\n","            model_path = os.path.join(output_directory, '{}.pkl'.format(ckpt_iter))\n","            checkpoint = torch.load(model_path, map_location='cpu')                   # [Zihao]: torch used\n","                                                            # 这里使用了torch去读取(开包)之前存储的checkpoint,属于pytorch的特定用法\n","            # feed model dict and optimizer state\n","            net.load_state_dict(checkpoint['model_state_dict'])                       # 这里也是torch,checkpoint['model_state_dict']里存储着之前生成的模型的已经存了的参数，是一个字典,这里是让net model读取每一个神经网络层的参数, load_state_dict是torch.nn.Module.load_state_dict(),就是为了读取nn模型的参数字典\n","            if 'optimizer_state_dict' in checkpoint:                           # deserialize 存储在checkpoint中,见下面的torch.save: model_state_dict': net.state_dict()，如果之前没有使用过optimizer模型进行迭代，则初始参数会使用net.state_dict(),\n","                optimizer.load_state_dict(checkpoint['optimizer_state_dict'])              # 当然如果我们之前进行过train的迭代，即使用过optimizer模型(Adam)，那么会产生新的参数，所以我们会使用下面optimizer.state_dict()存储的checkpoint['optimizer_state_dict']\n","\n","            print('Successfully loaded model at iteration {}'.format(ckpt_iter))           # 到这里才算是将checkpoint里面所有参数重新读取到新的模型中了\n","        except:\n","            ckpt_iter = -1\n","            print('No valid checkpoint model found, start training from initialization try.')   # 如果没有找到checkpoint，意味着要从头训练模型结构，这时可以看到手动将ckpt_iter设定为-1，为的是下面的train过程，不同的ckpt_iter会有不同的结果\n","    else:\n","        ckpt_iter = -1\n","        print('No valid checkpoint model found, start training from initialization.')\n","\n","        \n","        \n","    \n","    ### Custom data loading and reshaping ###\n","        \n","        \n","\n","    training_data = np.load(trainset_config['train_data_path'])  # 'train_data_path'        # trainset_config 在下面的初始化里面有，表示从imputer的json结构中获得train data的路径\n","    training_data = np.split(training_data, 160, 0)     # 这个将会把 training_data 分为160个相等分段(batch)，0表现axis=0，沿着横轴(时间)分割，training_data是3-D矩阵数据，第一维度是实验轨道次数：8000(0)，第二维度是每个轨道的100个样本时间点(1), 第三维度是14个features(2),所以这里表示分割为160份数据，features数目保持不变，时间保持不变，轨道分为160份，每份8000/160个轨道实验\n","    training_data = np.array(training_data)          # 转为array\n","    training_data = torch.from_numpy(training_data).float().cuda()                        # [Zihao]: torch used\n","    print('Data loaded')                                             # 这里的含义是将array转为一个pytorch下的tensor(转化为浮点数32),之后声明转化为cuda则表示将此tensor分配到gpu上\n","\n","    # training\n","    n_iter = ckpt_iter + 1                 # 这里的train.py的训练过程，如果在上面已经找到了之前训练过的checkpoint,则在上面已经赋给model了checkpoint内的神经网络层的参数，比如SSSDS4模型我们之前设定的\"n_iters\": 500 (在json文件中)，产生的checkpoint有600.pkl\n","                                  # 那么这里的ckpt_iter = 600, 则n_iter = 601>下面的n_iters+1 = 501 所以直接不进行while循环，直接跳出不再训练。\n","                                  # 如果在上面的checkpoint内没有发现.pkl记忆点文件，则上面会赋值ckpt_iter = -1, 此时n_iter = 0,则会进行下面的while循环进行训练，每次训练结束会给n_iter+1,当循环500次的时候，末尾再加1变成n_iter=501 = n_iters + 1(我们设定的),此时跳出循环完成训练\n","    while n_iter < n_iters + 1:\n","        for batch in training_data:           # 在上面我们将training_data分为了160组数据，那么应该就是160个batch，进行批量训练\n","\n","            if masking == 'rm':           # 这里就是看missing的数据类型是什么，可以是'rm': random missing, 'mnr': missing not at random, 'bm': blackout missing, 在SSSDS4里我们目前设定的是rm\n","                transposed_mask = get_mask_rm(batch[0], missing_k)  # get_mask_rm从util.py中import的,样本为batch[0]，这是随便选的第一个batch为例子，维度只有时间和feature两个, 因为在util.py里面我们只需要用到sample.shape，shape的大小数值而已\n","            elif masking == 'mnr':\n","                transposed_mask = get_mask_mnr(batch[0], missing_k)  # get_mask_mnr从util.py中import的,样本为batch[0]，这是随便选的第一个batch为例子，维度只有时间和feature两个, 因为在util.py里面我们只需要用到sample.shape，shape的大小数值而已\n","            elif masking == 'bm':\n","                transposed_mask = get_mask_bm(batch[0], missing_k)  # get_mask_bm从util.py中import的,样本为batch[0]，这是随便选的第一个batch为例子，维度只有时间和feature两个, 因为在util.py里面我们只需要用到sample.shape，shape的大小数值而已\n","\n","            mask = transposed_mask.permute(1, 0)             # transposed_mask是一个torch tensor,这个permute属性是将此tensor的维度转为transposed_mask原始维度(0,1)中，1变到现在的第一号位，0变到现在的第二号位置: (1,0),其实就是行变列，列变行，现在行有14个表示features，列有100个表示每个轨道的时间点\n","            mask = mask.repeat(batch.size()[0], 1, 1).float().cuda()   # torch tensor.repeat, 表示将原始mask重复，构成新的tensor,结构设定为重复batch第一维度：轨道数，以此来补充满每个batch内的所有轨道数: 8000/160,每列每行中一个元素: 0 or 1，batch是一个三维数组，后两维都repeat了1次说明保持不变，于是这里说明我们直接将mask加了一个维度即batch.size()[0](第一维度)\n","            loss_mask = ~mask.bool()  #将mask的值都转为bool值，0与1，之后使用~运算符表示：按位取反，本来是0的改为1，本来是1的改为0，之后叫做loss_mask\n","            batch = batch.permute(0, 2, 1) # 这里表示把batch的维度的后两个维度替换位置，现在第一维度轨道数保持不变，第二维度变成features，第三维度变成每个轨道的时间点数，此时和mask的三个维度重合\n","\n","            assert batch.size() == mask.size() == loss_mask.size()  # assert是一个检查维护过程，这里检查batch, mask和loss_mask的size(三个维度大小)是否都相等\n","\n","            # back-propagation\n","            optimizer.zero_grad()  # 表示在进行后向传播的时候的起点，0 gradient即0梯度\n","            X = batch, batch, mask, loss_mask  # 这里在输入要计算loss function的自变量X, X[0] = batch, X[1] = batch, X[2] = mask. X[3] = loss_mask\n","            loss = training_loss(net, nn.MSELoss(), X, diffusion_hyperparams,         # 这里使用的training_loss类在util.py文件中，返回的是loss值，即nn.MSE值，diffusion_hyperparams是计算出来的，来自util.py的calc_diffusion_hyperparams\n","                                 only_generate_missing=only_generate_missing)  # 注意最后的only_generate_missing，看上面的介绍，这个值如果是0则表示使用所有样本进行diffusion后获得MSE，如果是1则表示只使用missing部分进行diffusion获得的MSE\n","\n","            loss.backward()    # 目前没发现这里用backward()有什么具体作用，主要作用就是返回loss的值是多少，loss是一个tensor，有值\n","            optimizer.step()   # optimizer.step() 用于更新所有给定的参数，即给Adam最优化算法的参数： net.parameters()\n","\n","            if n_iter % iters_per_logging == 0:   # 默认是100，即当迭代次数是整百次的时候，我们这里print出已经进行了多少次迭代和loss值是多少\n","                print(\"iteration: {} \\tloss: {}\".format(n_iter, loss.item()))\n","\n","            # save checkpoint\n","            if n_iter > 0 and n_iter % iters_per_ckpt == 0:  # 在进行了整百次训练的时候：\n","                checkpoint_name = '{}.pkl'.format(n_iter)  #这是在给checkpoint文件(.pkl)进行命名\n","                torch.save({'model_state_dict': net.state_dict(),  #这是进行checkpoint文件的存储，按照字典的方式，存了未经过训练的初始化参数net.state_dict()，以及经过训练后的基于Adam最优算法迭代后的参数：optimizer.state_dict()\n","                            'optimizer_state_dict': optimizer.state_dict()},\n","                           os.path.join(output_directory, checkpoint_name))  # 存储路径为json文件内定义的output_directory，命名为checkpoint的文件名\n","                print('model at iteration %s is saved' % n_iter)  # print出checkpoint以及完成储存\n","\n","            n_iter += 1   # 迭代次数+1，进行下一次的迭代\n","\n","   "],"metadata":{"id":"-t0qKUmfaGu6","executionInfo":{"status":"ok","timestamp":1670430681954,"user_tz":0,"elapsed":1438,"user":{"displayName":"Zihao LIU","userId":"01384449044500819690"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["if __name__ == \"__main__\":                  # __name__ == \"__main__\" 表示目前run的就是train_self_written1_SSSDS4.ipynb文件本身，使用的模型(即train，上面block里定义的)都来自这个文件,这个代码块代表train()模型初始化时候的给定参数\n","    parser = argparse.ArgumentParser()\n","    parser.add_argument('-c', '--config', type=str, default='/content/drive/MyDrive/JPMorgan/自写代码/TensorFlow_codes/config/config_SSSDS4.json',  \n","                        help='JSON file for configuration')  # 这里只是表示读取文件的位置等，当run这个block的时候显示的比较好看\n","\n","    args = parser.parse_args(\"\")\n","\n","    with open(args.config) as f:   # 读取json参数内容\n","        data = f.read()   # 读取json参数内容\n","\n","    config = json.loads(data)  # 读取json参数内容， 将变量config赋值为json形式下的data\n","    print(config)   \n","\n","    train_config = config[\"train_config\"]  # training parameters  #训练模型的参数，来自对应imputer的json参数\n","\n","    global trainset_config\n","    trainset_config = config[\"trainset_config\"]  # to load trainset  # train和test数据的路径参数，来自对应imputer的json参数\n","\n","    global diffusion_config\n","    diffusion_config = config[\"diffusion_config\"]  # basic hyperparameters  # 这是diffusion模型的初始化超参数，来自对应imputer的json参数，之后会带入util.py文件的calc_diffusion_hyperparams去计算出扩散模型的超参数\n","\n","    global diffusion_hyperparams\n","    diffusion_hyperparams = calc_diffusion_hyperparams(\n","        **diffusion_config)  # dictionary of all diffusion hyperparameters  # 这里就是使用了util.py文件内的calc_diffusion_hyperparams,引入所有diffusion_config内的参数去计算出了扩散模型的超参数\n","\n","    global model_config\n","    if train_config['use_model'] == 0:         # 这些是imputer对应模型的初始化超参数，分别对应0:DiffWave. 1:SSSDSA. 2:SSSDS4.\n","        model_config = config['wavenet_config']   # 给model_config赋值这些参数，之后可以带入模型进行imputer初始化，方便后续使用Adam算法进行迭代，见上面\n","    elif train_config['use_model'] == 1:\n","        model_config = config['sashimi_config']\n","    elif train_config['use_model'] == 2:\n","        model_config = config['wavenet_config']\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FddY4GWp3xt_","executionInfo":{"status":"ok","timestamp":1670430739751,"user_tz":0,"elapsed":324,"user":{"displayName":"Zihao LIU","userId":"01384449044500819690"}},"outputId":"76f7cfd2-4ab7-4143-b9a4-90dd00841158"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["{'diffusion_config': {'T': 200, 'beta_0': 0.0001, 'beta_T': 0.02}, 'wavenet_config': {'in_channels': 14, 'out_channels': 14, 'num_res_layers': 36, 'res_channels': 256, 'skip_channels': 256, 'diffusion_step_embed_dim_in': 128, 'diffusion_step_embed_dim_mid': 512, 'diffusion_step_embed_dim_out': 512, 's4_lmax': 100, 's4_d_state': 64, 's4_dropout': 0.0, 's4_bidirectional': 1, 's4_layernorm': 1}, 'train_config': {'output_directory': '/content/drive/MyDrive/JPMorgan/自写代码/TensorFlow_codes/Results_SSSDS4/Mujoco/train_90', 'ckpt_iter': 'max', 'iters_per_ckpt': 100, 'iters_per_logging': 100, 'n_iters': 500, 'learning_rate': 0.0002, 'only_generate_missing': 1, 'use_model': 2, 'masking': 'rm', 'missing_k': 90}, 'trainset_config': {'train_data_path': '/content/drive/MyDrive/JPMorgan/自写代码/Data/Mujoco/train_mujoco.npy', 'test_data_path': '/content/drive/MyDrive/JPMorgan/自写代码/Data/Mujoco/test_mujoco.npy', 'segment_length': 100, 'sampling_rate': 100}, 'gen_config': {'output_directory': '/content/drive/MyDrive/JPMorgan/自写代码/TensorFlow_codes/Results_SSSDS4/Mujoco/test_90', 'ckpt_path': '/content/drive/MyDrive/JPMorgan/自写代码/TensorFlow_codes/Results_SSSDS4/Mujoco/train_90'}}\n"]}]},{"cell_type":"code","source":["# The start of the training.\n","# All the parameters are the already set ones in **train_config, it is from the json file we set, \n","# here is \"/content/drive/MyDrive/JPMorgan/自写代码/TensorFlow_codes/config/config_SSSDS4.json\",\n","# the \"**\" means the \"train\" model will accept all the parameters in the json file above. So by this way,\n","# all the already defined parameters in the json file will by inputted into the \"train\" model \n","\n","# It will take lots of time when you first run it as it will generate the iteration saving checkpoint. So next\n","# time when you run it, it will just start from the last iteration memory checkpoint. The path you can find is\n","# in the json file called: \"output_directory\"\n","\n","train(**train_config)"],"metadata":{"id":"a176XBiv4yqS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1670430767198,"user_tz":0,"elapsed":24885,"user":{"displayName":"Zihao LIU","userId":"01384449044500819690"}},"outputId":"d7184047-81ce-4ccd-dfad-193ade9f1d13"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["output directory /content/drive/MyDrive/JPMorgan/自写代码/TensorFlow_codes/Results_SSSDS4/Mujoco/train_90/T200_beta00.0001_betaT0.02\n","SSSDS4Imputer Parameters: 48.371726M\n","Successfully loaded model at iteration 600\n","Data loaded\n"]}]},{"cell_type":"code","source":["aaa = train(**train_config)"],"metadata":{"id":"fXZO93Bk4ytB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1668795642681,"user_tz":0,"elapsed":18952,"user":{"displayName":"Zihao LIU","userId":"01384449044500819690"}},"outputId":"7407e8f5-510f-489f-aaee-2d0a4174cf43"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["output directory ./results/mujoco/90/T200_beta00.0001_betaT0.02\n","SSSDS4Imputer Parameters: 48.371726M\n","Successfully loaded model at iteration 600\n","Data loaded\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"EN1PPh9f4y2a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"d0CFLHGJ4y4_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"E_w_NwWg4y7m"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Test whether GPU is using\n","!nvidia-smi"],"metadata":{"id":"HyluzmfJpld7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1668792214586,"user_tz":0,"elapsed":472,"user":{"displayName":"Zihao LIU","userId":"01384449044500819690"}},"outputId":"a2b2a816-926e-4d8e-8afb-4bae9c814465"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Fri Nov 18 17:23:34 2022       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   55C    P8    10W /  70W |      0MiB / 15109MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}]},{"cell_type":"code","source":["torch.cuda.is_available()\n"],"metadata":{"id":"DgjFqSXq3Umu","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1668794555987,"user_tz":0,"elapsed":517,"user":{"displayName":"Zihao LIU","userId":"01384449044500819690"}},"outputId":"82bea123-e3ee-4da8-96e4-f2db02d1bf7b"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":14}]},{"cell_type":"markdown","source":["————————————————————————————————————————\n","\n","The following is done by myself used for some exercises\n","\n","————————————————————————————————————————"],"metadata":{"id":"a50FB58cz1zu"}},{"cell_type":"code","source":["import torch"],"metadata":{"id":"eWrWbQK04eYm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["aaa=torch.tensor([2, 3, 5])"],"metadata":{"id":"RtAWGpDHy9vx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["aaa.repeat(3,5)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3F9AIS5JzGhl","executionInfo":{"status":"ok","timestamp":1670101431399,"user_tz":0,"elapsed":219,"user":{"displayName":"Zihao LIU","userId":"01384449044500819690"}},"outputId":"06605224-f5a2-4405-c66c-9c686e4e4807"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[2, 3, 5, 2, 3, 5, 2, 3, 5, 2, 3, 5, 2, 3, 5],\n","        [2, 3, 5, 2, 3, 5, 2, 3, 5, 2, 3, 5, 2, 3, 5],\n","        [2, 3, 5, 2, 3, 5, 2, 3, 5, 2, 3, 5, 2, 3, 5]])"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["aaa.repeat(3, 1, 2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jel_uPfVzIlz","executionInfo":{"status":"ok","timestamp":1670101438253,"user_tz":0,"elapsed":2,"user":{"displayName":"Zihao LIU","userId":"01384449044500819690"}},"outputId":"8d082063-fc7d-4517-fd12-6c3441dbd42b"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[2, 3, 5, 2, 3, 5]],\n","\n","        [[2, 3, 5, 2, 3, 5]],\n","\n","        [[2, 3, 5, 2, 3, 5]]])"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["aaa.repeat(3, 1, 2).size()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oEM0RAQ8zYw_","executionInfo":{"status":"ok","timestamp":1670101490210,"user_tz":0,"elapsed":3,"user":{"displayName":"Zihao LIU","userId":"01384449044500819690"}},"outputId":"01504969-7734-4621-f28a-82e09e4f92b1"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([3, 1, 6])"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["aaa.repeat(3,5).shape[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"goBpTvobzlh_","executionInfo":{"status":"ok","timestamp":1670101982052,"user_tz":0,"elapsed":5,"user":{"displayName":"Zihao LIU","userId":"01384449044500819690"}},"outputId":"c525bcc0-0d19-44bc-9013-f633ce8b9c77"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["3"]},"metadata":{},"execution_count":13}]},{"cell_type":"code","source":["data = np.load(\"/content/drive/MyDrive/JPMorgan/自写代码/Data/Mujoco/train_mujoco.npy\")"],"metadata":{"id":"RYwMqGKk1mKA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data"],"metadata":{"id":"u-tnB17F6p7M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["torch.manual_seed(0)\n","import torch\n","import torch.nn as nn\n","loss = nn.MSELoss()\n","input = torch.randn(3, 5, requires_grad=True)\n","target = torch.randn(3, 5)\n","output = loss(input, target)\n","output.backward()"],"metadata":{"id":"GOPxW0ta6rgU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["output"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DF4fAaHQlZMo","executionInfo":{"status":"ok","timestamp":1670198826523,"user_tz":0,"elapsed":227,"user":{"displayName":"Zihao LIU","userId":"01384449044500819690"}},"outputId":"5d19b8f6-badd-467f-fdd7-011734b4044a"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(1.6508, grad_fn=<MseLossBackward0>)"]},"metadata":{},"execution_count":26}]},{"cell_type":"code","source":["import tensorflow as tf\n","data = (2,3,2,4)\n","x = tf.random.normal(data[-3:])\n","x"],"metadata":{"id":"691YClM1l0Kp","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1670452385574,"user_tz":0,"elapsed":2,"user":{"displayName":"Zihao LIU","userId":"01384449044500819690"}},"outputId":"0a995394-8e7a-4cc1-a956-465ef15ced71"},"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(3, 2, 4), dtype=float32, numpy=\n","array([[[-1.8512957 , -0.10105247, -0.01942298, -0.64788115],\n","        [-1.4454846 , -1.2928028 ,  0.61011493,  0.59740067]],\n","\n","       [[ 2.7605448 ,  1.2194499 ,  0.9114438 ,  0.97699237],\n","        [ 0.05329418,  0.71749556,  0.7032246 , -0.11779961]],\n","\n","       [[ 1.8865706 ,  1.7269831 , -0.06335431, -1.1300508 ],\n","        [ 1.6945406 ,  0.6119375 , -1.2051554 ,  0.09572101]]],\n","      dtype=float32)>"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":[],"metadata":{"id":"LDCLJsZPtu-R"},"execution_count":null,"outputs":[]}]}